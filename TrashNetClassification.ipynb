{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15T36lSVfj3o"
      },
      "source": [
        "## Load and Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:38:53.933362Z",
          "iopub.status.busy": "2024-12-21T09:38:53.933078Z",
          "iopub.status.idle": "2024-12-21T09:39:01.531224Z",
          "shell.execute_reply": "2024-12-21T09:39:01.530346Z",
          "shell.execute_reply.started": "2024-12-21T09:38:53.933339Z"
        },
        "id": "iw87Qv7OTR-o",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import transforms\n",
        "\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:39:01.532938Z",
          "iopub.status.busy": "2024-12-21T09:39:01.532701Z",
          "iopub.status.idle": "2024-12-21T09:39:26.436110Z",
          "shell.execute_reply": "2024-12-21T09:39:26.435479Z",
          "shell.execute_reply.started": "2024-12-21T09:39:01.532915Z"
        },
        "id": "1zcvPIIvTR-q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"garythung/trashnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:39:26.437631Z",
          "iopub.status.busy": "2024-12-21T09:39:26.437328Z",
          "iopub.status.idle": "2024-12-21T09:39:26.441905Z",
          "shell.execute_reply": "2024-12-21T09:39:26.441336Z",
          "shell.execute_reply.started": "2024-12-21T09:39:26.437601Z"
        },
        "id": "5XAdH8UiTR-q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:39:26.442851Z",
          "iopub.status.busy": "2024-12-21T09:39:26.442589Z",
          "iopub.status.idle": "2024-12-21T09:39:30.593185Z",
          "shell.execute_reply": "2024-12-21T09:39:30.592321Z",
          "shell.execute_reply.started": "2024-12-21T09:39:26.442825Z"
        },
        "id": "oh-eU7T4TR-r",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "labels = ds['train']['label']\n",
        "unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "class_distribution = dict(zip(unique_labels, counts))\n",
        "print(\"\\nclass distribution:\", class_distribution)\n",
        "\n",
        "plt.bar(unique_labels, counts)\n",
        "plt.xlabel(\"Labels\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:39:30.594277Z",
          "iopub.status.busy": "2024-12-21T09:39:30.593965Z",
          "iopub.status.idle": "2024-12-21T09:39:30.661350Z",
          "shell.execute_reply": "2024-12-21T09:39:30.660579Z",
          "shell.execute_reply.started": "2024-12-21T09:39:30.594246Z"
        },
        "id": "hpHYmg9tTR-r",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "total_samples = len(ds['train'])\n",
        "num_classes = len(unique_labels)\n",
        "\n",
        "print(\"\\nTotal samples:\", total_samples)\n",
        "print(\"Number of classes:\", num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:39:30.663313Z",
          "iopub.status.busy": "2024-12-21T09:39:30.662988Z",
          "iopub.status.idle": "2024-12-21T09:39:35.855990Z",
          "shell.execute_reply": "2024-12-21T09:39:35.855291Z",
          "shell.execute_reply.started": "2024-12-21T09:39:30.663289Z"
        },
        "id": "vx3kZDYtTR-s",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "unique_sizes = set()\n",
        "total_images = len(ds['train'])\n",
        "\n",
        "for i in range(0, total_images, 100):\n",
        "    img = ds['train'][i]['image']\n",
        "    unique_sizes.add(img.size)\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Processed {i}/{total_images} images\")\n",
        "\n",
        "print(\"\\nunique sizes of image:\", list(unique_sizes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipH_-ArVcESU"
      },
      "source": [
        "Handling Image Sizes:\n",
        "Handling a variety of image sizes involves transforming all images into uniform dimensions, in this case,\n",
        "224\n",
        "Ã—\n",
        "224\n",
        ". This reduces computational memory and time. However, on the downside, it may lead to a significant loss of information from the images (especially those larger than 1000 pixels). Therefore, having ample computational resources would be beneficial for processing larger datasets, which could better represent real-world scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRsCrsCOTR-t"
      },
      "source": [
        "Handling Imbalanced Data:\n",
        "To address the issue of imbalanced data, particularly for class number '5,' we apply a weighting strategy in the CrossEntropyLoss function. The weights are calculated inversely proportional to the class frequencies, ensuring that the model assigns greater importance to the minority class during training.\n",
        "Although the primary goal is to achieve high accuracy for this dataset, this approach may still have limitations when applied to real-world scenarios. Real-world cases often require a more diverse and extensive dataset to develop a model that achieves higher accuracy and robustness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmy8v5lYfvtd"
      },
      "source": [
        "### Split Dataset into Training and Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCNOSBvmf31Q"
      },
      "source": [
        "Since, the number of dataset is not too large to train, that's why the dataset splitted only into 2 set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:39:35.857199Z",
          "iopub.status.busy": "2024-12-21T09:39:35.856959Z",
          "iopub.status.idle": "2024-12-21T09:39:35.870455Z",
          "shell.execute_reply": "2024-12-21T09:39:35.869597Z",
          "shell.execute_reply.started": "2024-12-21T09:39:35.857175Z"
        },
        "id": "EzbWOocGTR-u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ds_index = list(range(len(ds['train'])))\n",
        "train_index, val_index = train_test_split(\n",
        "    ds_index, test_size=0.2, random_state=42, stratify=ds['train']['label'])\n",
        "\n",
        "train_dataset = Subset(ds['train'], train_index)\n",
        "val_dataset = Subset(ds['train'], val_index)\n",
        "print(\"train size:\", len(train_dataset))\n",
        "print(\"validation size:\", len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YvPulYmgG6s"
      },
      "source": [
        "### Preprocessing Train and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:39:35.871635Z",
          "iopub.status.busy": "2024-12-21T09:39:35.871324Z",
          "iopub.status.idle": "2024-12-21T09:39:35.886600Z",
          "shell.execute_reply": "2024-12-21T09:39:35.885798Z",
          "shell.execute_reply.started": "2024-12-21T09:39:35.871610Z"
        },
        "id": "6Pk3xZbZTR-v",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:39:35.887630Z",
          "iopub.status.busy": "2024-12-21T09:39:35.887406Z",
          "iopub.status.idle": "2024-12-21T09:39:35.906865Z",
          "shell.execute_reply": "2024-12-21T09:39:35.906178Z",
          "shell.execute_reply.started": "2024-12-21T09:39:35.887610Z"
        },
        "id": "VGR3sNIdTR-w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class SimpleTrashDataset(Dataset):\n",
        "    def __init__(self, dataset, indices, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.dataset[self.indices[idx]]['image']\n",
        "        label = self.dataset[self.indices[idx]]['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "train_dataset = SimpleTrashDataset(ds['train'], train_index, train_transform)\n",
        "val_dataset = SimpleTrashDataset(ds['train'], val_index, val_transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T12:03:16.394861Z",
          "iopub.status.busy": "2024-12-21T12:03:16.394535Z",
          "iopub.status.idle": "2024-12-21T12:04:04.349667Z",
          "shell.execute_reply": "2024-12-21T12:04:04.348519Z",
          "shell.execute_reply.started": "2024-12-21T12:03:16.394834Z"
        },
        "id": "2EkfAP_tTR-w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    print(f\"Batch size: {images.size(0)}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cT4e908gSXw"
      },
      "source": [
        "## Setup the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxPgfCjAgZVV"
      },
      "source": [
        "Since this notebook will runned on GitHub Action, the chosen model is not too large because it will use CPU system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T12:04:04.351506Z",
          "iopub.status.busy": "2024-12-21T12:04:04.351156Z",
          "iopub.status.idle": "2024-12-21T12:04:04.831879Z",
          "shell.execute_reply": "2024-12-21T12:04:04.831159Z",
          "shell.execute_reply.started": "2024-12-21T12:04:04.351468Z"
        },
        "id": "ZFDNTNsqTR-w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "num_classes = 6\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for layer in [model.layer4]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, num_classes)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE5LQu9OdPgx"
      },
      "source": [
        "This is how imbalance class distribution looks like in a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T12:04:26.869477Z",
          "iopub.status.busy": "2024-12-21T12:04:26.869168Z",
          "iopub.status.idle": "2024-12-21T12:05:14.769573Z",
          "shell.execute_reply": "2024-12-21T12:05:14.767967Z",
          "shell.execute_reply.started": "2024-12-21T12:04:26.869454Z"
        },
        "id": "z-mBLg8ETR-w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for images, labels in train_loader:\n",
        "    label_counts = Counter(labels.tolist())\n",
        "    print(f\"Batch class distribution: {label_counts}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4WgCvwWjveX"
      },
      "source": [
        "Apply the inverse method in CrossEntropyLoss to adjust weight of underrepresented class.This weighting strategy ensures that the minority class (class number '5' in this case) contributes proportionally more to the loss function, effectively addressing the imbalance and encouraging the model to pay more attention to underrepresented classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T12:05:14.771566Z",
          "iopub.status.busy": "2024-12-21T12:05:14.771062Z",
          "iopub.status.idle": "2024-12-21T12:05:14.921730Z",
          "shell.execute_reply": "2024-12-21T12:05:14.920898Z",
          "shell.execute_reply.started": "2024-12-21T12:05:14.771514Z"
        },
        "id": "tMEttJUWTR-w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_labels = ds['train'].select(train_index)['label']\n",
        "\n",
        "class_counts = np.bincount(train_labels)\n",
        "class_weights = 1.0 / class_counts\n",
        "class_weights = class_weights / class_weights.sum() * len(class_counts)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "print(f\"Class Weights: {class_weights}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "class_weights = class_weights.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T12:05:14.923461Z",
          "iopub.status.busy": "2024-12-21T12:05:14.923175Z",
          "iopub.status.idle": "2024-12-21T12:05:14.928081Z",
          "shell.execute_reply": "2024-12-21T12:05:14.927188Z",
          "shell.execute_reply.started": "2024-12-21T12:05:14.923437Z"
        },
        "id": "oZSfhpjtTR-x",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Class weights device: {class_weights.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T09:40:58.825224Z",
          "iopub.status.busy": "2024-12-21T09:40:58.824911Z",
          "iopub.status.idle": "2024-12-21T09:41:01.278792Z",
          "shell.execute_reply": "2024-12-21T09:41:01.277905Z",
          "shell.execute_reply.started": "2024-12-21T09:40:58.825198Z"
        },
        "id": "cuBxEjmqTR-x",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kZB6BuojJ6D"
      },
      "source": [
        "### Setup W&B Hyperparameter Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T12:46:33.588123Z",
          "iopub.status.busy": "2024-12-21T12:46:33.587808Z",
          "iopub.status.idle": "2024-12-21T12:46:33.592236Z",
          "shell.execute_reply": "2024-12-21T12:46:33.591382Z",
          "shell.execute_reply.started": "2024-12-21T12:46:33.588073Z"
        },
        "id": "zJpxXnJqTR-x",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "configs = {\n",
        "        \"learning_rate\": 0.0001,\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"dropout_rate\": 0.4,\n",
        "        \"epochs\": 12,\n",
        "        \"num_classes\":6}\n",
        "\n",
        "if configs[\"optimizer\"] == 'adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=configs[\"learning_rate\"])\n",
        "elif configs[\"optimizer\"] == 'sgd':\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=configs[\"learning_rate\"],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T12:46:47.674091Z",
          "iopub.status.busy": "2024-12-21T12:46:47.673798Z"
        },
        "id": "P2-LTNXPTR-x",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"trash-image-classification\",\n",
        "    name=f\"newrun_resnet18\",\n",
        "    config=configs,\n",
        "    notes=\"Training ResNet with class weights\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEnzasNejVM_"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-21T12:44:01.836764Z",
          "iopub.status.busy": "2024-12-21T12:44:01.836433Z",
          "iopub.status.idle": "2024-12-21T12:44:07.170639Z",
          "shell.execute_reply": "2024-12-21T12:44:07.169935Z",
          "shell.execute_reply.started": "2024-12-21T12:44:01.836734Z"
        },
        "id": "iElN94yiTR-z",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "best_acc = 0.0\n",
        "for epoch in range(configs[\"epochs\"]):\n",
        "        print(f'\\nEpoch {epoch+1}/{configs[\"epochs\"]}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        epoch_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            if batch_idx % 20 == 19:\n",
        "                avg_loss = running_loss / 20\n",
        "                train_accuracy = 100 * correct_train / total_train\n",
        "                print(f'Batch {batch_idx+1}, Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.2f}%')\n",
        "                wandb.log({\n",
        "                    \"batch_loss\": avg_loss,\n",
        "                    \"epoch\": epoch,\n",
        "                    \"batch\": batch_idx,\n",
        "                    \"batch_accuracy\": train_accuracy\n",
        "                })\n",
        "                running_loss = 0.0\n",
        "\n",
        "        epoch_loss = epoch_loss / len(train_loader)\n",
        "        epoch_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        class_correct = {i: 0 for i in range(configs[\"num_classes\"])}\n",
        "        class_total = {i: 0 for i in range(configs[\"num_classes\"])}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                for label, pred in zip(labels, predicted):\n",
        "                    if label == pred:\n",
        "                        class_correct[label.item()] += 1\n",
        "                    class_total[label.item()] += 1\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = 100 * correct / total\n",
        "        class_accuracies = {\n",
        "            f\"class_{i}\": 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "            for i in range(configs[\"num_classes\"])\n",
        "        }\n",
        "\n",
        "        print(f'\\nEpoch Summary:')\n",
        "        print(f'Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.2f}%')\n",
        "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "        for class_idx, acc in class_accuracies.items():\n",
        "            print(f\"{class_idx}: {acc:.2f}%\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": epoch_loss,\n",
        "            \"train_accuracy\": epoch_accuracy,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_accuracy,\n",
        "            **{f\"class_{k}_accuracy\": v for k, v in class_accuracies.items()}\n",
        "        })\n",
        "\n",
        "        if val_accuracy > best_acc:\n",
        "            best_acc = val_accuracy\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'accuracy': val_accuracy,\n",
        "                'model_config': {\n",
        "                    'architecture': 'resnet18',\n",
        "                    'num_classes': 6,\n",
        "                    'class_names': ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash'],\n",
        "                    'image_size': 224,  \n",
        "                    'pretrained': True,  \n",
        "                    'training_params': {\n",
        "                        'batch_size': 32,\n",
        "                        'learning_rate': optimizer.param_groups[0]['lr'],\n",
        "                        'epochs': configs['epochs']\n",
        "                    }\n",
        "                }\n",
        "            }, 'best_model.pth')\n",
        "\n",
        "            if wandb.run:\n",
        "                wandb.save('best_model.pth')\n",
        "            print(f\"\\nCheckpoint saved: 'best_model.pth'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hktaefTSvWpQ"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CGPJcByjYf4"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1FFasq-QaK4"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('best_model.pth', map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbDceBOGRk2n"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "test_accuracy = 100 * test_correct / test_total\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjV6gLerSBu6"
      },
      "outputs": [],
      "source": [
        "class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0B5DdWPT_7P"
      },
      "outputs": [],
      "source": [
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30822,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
